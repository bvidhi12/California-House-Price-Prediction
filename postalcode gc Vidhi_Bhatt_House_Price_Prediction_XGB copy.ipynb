{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A6HSgy1CepdO"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "600xmBFkpOXk",
        "outputId": "ded4abbe-da5f-4698-b79a-08f22baa9e4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v9/47dbywsd3kq7y4_mgpqb1kqm0000gn/T/ipykernel_3524/1710713689.py:6: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df6 = pd.read_csv('CRMLSSold202506_filled.csv')\n"
          ]
        }
      ],
      "source": [
        "df1 = pd.read_csv('CRMLSSold202501_filled.csv')\n",
        "df2 = pd.read_csv('CRMLSSold202502_filled.csv')\n",
        "df3 = pd.read_csv('CRMLSSold202503_filled.csv')\n",
        "df4 = pd.read_csv('CRMLSSold202504_filled.csv')\n",
        "df5 = pd.read_csv('CRMLSSold202505_filled.csv')\n",
        "df6 = pd.read_csv('CRMLSSold202506_filled.csv')\n",
        "df7 = pd.read_csv('CRMLSSold202507_filled.csv')\n",
        "df8 = pd.read_csv('CRMLSSold202508_filled-2.csv')\n",
        "df9 = pd.read_csv('CRMLSSold202509.csv')\n",
        "\n",
        "df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8,df9], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cXodC2HWV50"
      },
      "source": [
        "**FILTERING FOR PROPERTY TYPE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vxmp2_Gtp217",
        "outputId": "26364c7b-462b-47da-ce37-ed839402a7b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(97987, 80)\n"
          ]
        }
      ],
      "source": [
        "df = df[\n",
        "    (df[\"PropertyType\"] == \"Residential\") &\n",
        "    (df[\"PropertySubType\"] == \"SingleFamilyResidence\")\n",
        "]\n",
        "\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqNoz2zLWbFx"
      },
      "source": [
        "**DROPPING OF COLUMNS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeWh8N-jqnSs",
        "outputId": "84ed2a1c-8368-47ce-82a0-dd64e5ef91b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['BuyerAgentAOR', 'ListAgentAOR', 'Flooring', 'ViewYN', 'WaterfrontYN',\n",
              "       'BasementYN', 'PoolPrivateYN', 'OriginalListPrice', 'ListingKey',\n",
              "       'ListAgentEmail', 'CloseDate', 'ClosePrice', 'ListAgentFirstName',\n",
              "       'ListAgentLastName', 'Latitude', 'Longitude', 'UnparsedAddress',\n",
              "       'PropertyType', 'LivingArea', 'ListPrice', 'DaysOnMarket',\n",
              "       'ListOfficeName', 'BuyerOfficeName', 'CoListOfficeName',\n",
              "       'ListAgentFullName', 'CoListAgentFirstName', 'CoListAgentLastName',\n",
              "       'BuyerAgentMlsId', 'BuyerAgentFirstName', 'BuyerAgentLastName',\n",
              "       'FireplacesTotal', 'AssociationFeeFrequency', 'AboveGradeFinishedArea',\n",
              "       'ListingKeyNumeric', 'MLSAreaMajor', 'TaxAnnualAmount',\n",
              "       'CountyOrParish', 'MlsStatus', 'ElementarySchool', 'AttachedGarageYN',\n",
              "       'ParkingTotal', 'BuilderName', 'PropertySubType', 'LotSizeAcres',\n",
              "       'SubdivisionName', 'BuyerOfficeAOR', 'YearBuilt', 'StreetNumberNumeric',\n",
              "       'ListingId', 'BathroomsTotalInteger', 'City', 'TaxYear',\n",
              "       'BuildingAreaTotal', 'BedroomsTotal', 'ContractStatusChangeDate',\n",
              "       'ElementarySchoolDistrict', 'CoBuyerAgentFirstName',\n",
              "       'PurchaseContractDate', 'ListingContractDate', 'BelowGradeFinishedArea',\n",
              "       'BusinessType', 'StateOrProvince', 'CoveredSpaces',\n",
              "       'MiddleOrJuniorSchool', 'FireplaceYN', 'Stories', 'HighSchool',\n",
              "       'Levels', 'LotSizeDimensions', 'LotSizeArea', 'MainLevelBedrooms',\n",
              "       'NewConstructionYN', 'GarageSpaces', 'HighSchoolDistrict', 'PostalCode',\n",
              "       'AssociationFee', 'LotSizeSquareFeet', 'MiddleOrJuniorSchoolDistrict',\n",
              "       'latfilled', 'lonfilled'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "B7eMz5odrXXo"
      },
      "outputs": [],
      "source": [
        "#columns to drop\n",
        "drop_cols = [\n",
        "    \"ListPrice\",\"OriginalListPrice\",\"DaysOnMarket\",\"ListingContractDate\",\n",
        "    \"ContractStatusChangeDate\",\"PurchaseContractDate\",\n",
        "    \"ListAgentAOR\",\"ListAgentEmail\",\"ListAgentFirstName\",\"ListAgentLastName\",\n",
        "    \"ListAgentFullName\",\"CoListAgentFirstName\",\"CoListAgentLastName\",\n",
        "    \"BuyerAgentAOR\",\"BuyerAgentMlsId\",\"BuyerAgentFirstName\",\"BuyerAgentLastName\",\n",
        "    \"CoBuyerAgentFirstName\",\"BuyerOfficeAOR\",\"ListOfficeName\",\"BuyerOfficeName\",\n",
        "    \"CoListOfficeName\",\"MlsStatus\",\"ListingKey\",\"ListingId\",\"ListingKeyNumeric\",\n",
        "    \"AssociationFeeFrequency\", \"MLSAreaMajor\", \"BusinessType\",\"latfilled\",\"lonfilled\"\n",
        "]\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TvnKvVHtl1o"
      },
      "source": [
        "**Evaluating Missing values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q4kkhOx7tIfw",
        "outputId": "9b6ae6ef-2897-425f-ce46-3ae0d005aed2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MiddleOrJuniorSchoolDistrict    1.000000\n",
              "CoveredSpaces                   1.000000\n",
              "TaxYear                         1.000000\n",
              "TaxAnnualAmount                 1.000000\n",
              "AboveGradeFinishedArea          1.000000\n",
              "FireplacesTotal                 1.000000\n",
              "ElementarySchoolDistrict        1.000000\n",
              "WaterfrontYN                    0.999500\n",
              "BelowGradeFinishedArea          0.993224\n",
              "BasementYN                      0.975936\n",
              "BuilderName                     0.953167\n",
              "LotSizeDimensions               0.937135\n",
              "BuildingAreaTotal               0.933644\n",
              "ElementarySchool                0.866135\n",
              "MiddleOrJuniorSchool            0.864962\n",
              "HighSchool                      0.824650\n",
              "SubdivisionName                 0.647964\n",
              "MainLevelBedrooms               0.394287\n",
              "Flooring                        0.354363\n",
              "AssociationFee                  0.294427\n",
              "HighSchoolDistrict              0.268566\n",
              "AttachedGarageYN                0.119546\n",
              "Stories                         0.110780\n",
              "ViewYN                          0.090308\n",
              "PoolPrivateYN                   0.080388\n",
              "Levels                          0.076643\n",
              "NewConstructionYN               0.075581\n",
              "GarageSpaces                    0.039811\n",
              "LotSizeAcres                    0.017625\n",
              "LotSizeSquareFeet               0.017625\n",
              "LotSizeArea                     0.017533\n",
              "StreetNumberNumeric             0.001143\n",
              "UnparsedAddress                 0.000939\n",
              "YearBuilt                       0.000755\n",
              "City                            0.000714\n",
              "FireplaceYN                     0.000663\n",
              "LivingArea                      0.000592\n",
              "BathroomsTotalInteger           0.000194\n",
              "Latitude                        0.000061\n",
              "Longitude                       0.000061\n",
              "PostalCode                      0.000010\n",
              "ParkingTotal                    0.000010\n",
              "CloseDate                       0.000000\n",
              "ClosePrice                      0.000000\n",
              "PropertySubType                 0.000000\n",
              "StateOrProvince                 0.000000\n",
              "PropertyType                    0.000000\n",
              "BedroomsTotal                   0.000000\n",
              "CountyOrParish                  0.000000\n",
              "dtype: float64"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checking for missing values by column\n",
        "missing_percentage = df.isnull().mean().sort_values(ascending=False)\n",
        "missing_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SY2aNr-tyvc",
        "outputId": "874220cb-4067-4a55-8852-36217f29abe4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['MiddleOrJuniorSchoolDistrict',\n",
              " 'CoveredSpaces',\n",
              " 'TaxYear',\n",
              " 'TaxAnnualAmount',\n",
              " 'AboveGradeFinishedArea',\n",
              " 'FireplacesTotal',\n",
              " 'ElementarySchoolDistrict',\n",
              " 'WaterfrontYN',\n",
              " 'BelowGradeFinishedArea',\n",
              " 'BasementYN',\n",
              " 'BuilderName',\n",
              " 'LotSizeDimensions',\n",
              " 'BuildingAreaTotal',\n",
              " 'ElementarySchool',\n",
              " 'MiddleOrJuniorSchool',\n",
              " 'HighSchool',\n",
              " 'SubdivisionName']"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cols_to_drop = missing_percentage[missing_percentage > 0.6].index.tolist()\n",
        "cols_to_drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "_ElhsRDfOR6z"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=cols_to_drop, errors = 'ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSufdmruWoV5"
      },
      "source": [
        "**NULL VALUE COUNT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V4hxl8Ul8jRH",
        "outputId": "e5055848-4662-4333-b19f-696d6f4ae5ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MainLevelBedrooms        38635\n",
              "Flooring                 34723\n",
              "AssociationFee           28850\n",
              "HighSchoolDistrict       26316\n",
              "AttachedGarageYN         11714\n",
              "Stories                  10855\n",
              "ViewYN                    8849\n",
              "PoolPrivateYN             7877\n",
              "Levels                    7510\n",
              "NewConstructionYN         7406\n",
              "GarageSpaces              3901\n",
              "LotSizeAcres              1727\n",
              "LotSizeSquareFeet         1727\n",
              "LotSizeArea               1718\n",
              "StreetNumberNumeric        112\n",
              "UnparsedAddress             92\n",
              "YearBuilt                   74\n",
              "City                        70\n",
              "FireplaceYN                 65\n",
              "LivingArea                  58\n",
              "BathroomsTotalInteger       19\n",
              "Longitude                    6\n",
              "Latitude                     6\n",
              "ParkingTotal                 1\n",
              "PostalCode                   1\n",
              "StateOrProvince              0\n",
              "BedroomsTotal                0\n",
              "PropertyType                 0\n",
              "CountyOrParish               0\n",
              "ClosePrice                   0\n",
              "CloseDate                    0\n",
              "PropertySubType              0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxkcIfISW1GG"
      },
      "source": [
        "**DEALING W BOOLEAN NULL VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toXYWYjU9Qxo",
        "outputId": "6f25db60-a666-4f96-941e-be577b3f7360"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v9/47dbywsd3kq7y4_mgpqb1kqm0000gn/T/ipykernel_3524/3452942020.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[bool_cols] = df[bool_cols].fillna(False)\n"
          ]
        }
      ],
      "source": [
        "bool_cols = [\"ViewYN\", \"PoolPrivateYN\", \"AttachedGarageYN\",\n",
        "             \"FireplaceYN\",\"NewConstructionYN\"]\n",
        "\n",
        "df[bool_cols] = df[bool_cols].fillna(False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-Rexh5RW5SW"
      },
      "source": [
        "**SEPARATING THE NUM AND CATE COLUMNS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovTKZ7PVAMa0",
        "outputId": "aa6859f7-c3cc-425f-c3e5-e172de9cd3bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Latitude',\n",
              " 'Longitude',\n",
              " 'LivingArea',\n",
              " 'ParkingTotal',\n",
              " 'LotSizeAcres',\n",
              " 'YearBuilt',\n",
              " 'StreetNumberNumeric',\n",
              " 'BathroomsTotalInteger',\n",
              " 'BedroomsTotal',\n",
              " 'Stories',\n",
              " 'LotSizeArea',\n",
              " 'MainLevelBedrooms',\n",
              " 'GarageSpaces',\n",
              " 'AssociationFee',\n",
              " 'LotSizeSquareFeet']"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "numerical_cols = df.select_dtypes(include=['int64','float64']).columns.tolist() \n",
        "num_cols = [col for col in numerical_cols if col != 'ClosePrice']\n",
        "num_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET99YiVH5uzV"
      },
      "source": [
        "**SEPARATING TRAIN AND TEST USING CLOSE DATE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "oWhMgyYg5t_b"
      },
      "outputs": [],
      "source": [
        "df[\"CloseDate\"] = pd.to_datetime(df[\"CloseDate\"], errors=\"coerce\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29NFPFRI7QyT",
        "outputId": "b99631d6-028e-4b61-8d81-bde2ed1457ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "datetime64[ns]\n"
          ]
        }
      ],
      "source": [
        "print(df[\"CloseDate\"].dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "xyCSi_gW6yfO"
      },
      "outputs": [],
      "source": [
        "mask = (df[\"CloseDate\"].dt.year == 2025) & (df[\"CloseDate\"].dt.month == 9)\n",
        "\n",
        "test_df = df[mask].copy()\n",
        "train_df = df[~mask].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDCD9BG3XKPi"
      },
      "source": [
        "**IMPUTATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "6nsceMYsVsSF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "cols = [\"LivingArea\",\"AssociationFee\"]\n",
        "\n",
        "scaler = RobustScaler()\n",
        "imp = KNNImputer(n_neighbors=7, weights=\"distance\")\n",
        "\n",
        "# fit on train\n",
        "X_train = scaler.fit_transform(train_df[cols])\n",
        "X_train_imp = imp.fit_transform(X_train)\n",
        "train_df[cols] = scaler.inverse_transform(X_train_imp)\n",
        "\n",
        "# transform test with same fit\n",
        "X_test = scaler.transform(test_df[cols])\n",
        "X_test_imp = imp.transform(X_test)\n",
        "test_df[cols] = scaler.inverse_transform(X_test_imp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "RayJMMdFcM20"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "\n",
        "train_df[\"Stories\"] = imputer.fit_transform(train_df[[\"Stories\"]]).ravel()\n",
        "\n",
        "test_df[\"Stories\"] = imputer.transform(test_df[[\"Stories\"]]).ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W3KNCobdN4O",
        "outputId": "68784810-bae0-4721-a19f-94328de1254d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v9/47dbywsd3kq7y4_mgpqb1kqm0000gn/T/ipykernel_3524/2410216566.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  test_df[\"Levels\"].fillna(mode_val, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# --- Train ---\n",
        "s_train = pd.to_numeric(train_df[\"Stories\"], errors=\"coerce\")\n",
        "fill_train = s_train.map({1: \"One\", 2: \"Two\"})\n",
        "\n",
        "miss_train = train_df[\"Levels\"].isna()\n",
        "train_df.loc[miss_train, \"Levels\"] = fill_train[miss_train]\n",
        "\n",
        "# fallback to mode from train only\n",
        "if train_df[\"Levels\"].isna().any():\n",
        "    mode_val = train_df[\"Levels\"].dropna().mode().iloc[0]\n",
        "    train_df[\"Levels\"].fillna(mode_val, inplace=True)\n",
        "else:\n",
        "    mode_val = train_df[\"Levels\"].dropna().mode().iloc[0]\n",
        "\n",
        "train_df[\"Levels\"] = train_df[\"Levels\"].astype(\"category\")\n",
        "\n",
        "# --- Test ---\n",
        "s_test = pd.to_numeric(test_df[\"Stories\"], errors=\"coerce\")\n",
        "fill_test = s_test.map({1: \"One\", 2: \"Two\"})\n",
        "\n",
        "miss_test = test_df[\"Levels\"].isna()\n",
        "test_df.loc[miss_test, \"Levels\"] = fill_test[miss_test]\n",
        "\n",
        "# use mode derived from train_df\n",
        "test_df[\"Levels\"].fillna(mode_val, inplace=True)\n",
        "test_df[\"Levels\"] = test_df[\"Levels\"].astype(\"category\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "Mf0CzBLvE21k"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "num_cols = [\"YearBuilt\", \"BathroomsTotalInteger\", \"BedroomsTotal\",\n",
        "            \"ParkingTotal\", \"MainLevelBedrooms\", \"GarageSpaces\"]\n",
        "\n",
        "num_imputer = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "# fit only on train\n",
        "train_df[num_cols] = num_imputer.fit_transform(train_df[num_cols])\n",
        "test_df[num_cols]  = num_imputer.transform(test_df[num_cols])\n",
        "\n",
        "# round and cast\n",
        "for c in num_cols:\n",
        "    train_df[c] = train_df[c].round().astype(\"Int64\")\n",
        "    test_df[c]  = test_df[c].round().astype(\"Int64\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "8_CyoJkJjAjC"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.dropna(subset=[\"StreetNumberNumeric\",\"Latitude\",\"Longitude\",\"UnparsedAddress\",\"City\",\"PostalCode\"]).copy()\n",
        "test_df = test_df.dropna(subset=[\"StreetNumberNumeric\",\"Latitude\",\"Longitude\",\"UnparsedAddress\",\"City\",\"PostalCode\"]).copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "knhm-ZHplefa"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# materials = [\"Bamboo\",\"Brick\",\"Carpet\",\"Concrete\",\"Laminate\",\"Stone\",\"Tile\",\"Vinyl\",\"Wood\"]\n",
        "\n",
        "# def parse_flooring(s):\n",
        "#     if pd.isna(s):\n",
        "#         return []\n",
        "#     toks = [t.strip().title() for t in str(s).split(\",\") if t.strip()]\n",
        "#     bad = {\"Seeremarks\",\"See Remarks\",\"See-Remarks\"}\n",
        "#     return [t for t in toks if t not in bad]\n",
        "\n",
        "# # --- train ---\n",
        "# lists_train = train_df[\"Flooring\"].map(parse_flooring)\n",
        "# mlb = MultiLabelBinarizer(classes=materials)\n",
        "# X_train = mlb.fit_transform(lists_train)\n",
        "# train_df[materials] = pd.DataFrame(X_train, index=train_df.index, columns=materials).astype(\"int8\")\n",
        "\n",
        "# # --- test ---\n",
        "# lists_test = test_df[\"Flooring\"].map(parse_flooring)\n",
        "# X_test = mlb.transform(lists_test)\n",
        "# test_df[materials] = pd.DataFrame(X_test, index=test_df.index, columns=materials).astype(\"int8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 0) Clean PostalCode\n",
        "for d in (train_df, test_df):\n",
        "    d['PostalCode'] = d['PostalCode'].astype(str).str.strip().str.upper().replace({'': np.nan})\n",
        "\n",
        "# 1) PostalCode → median target (TRAIN ONLY)\n",
        "pc_price = (\n",
        "    train_df.loc[train_df['PostalCode'].notna(), ['PostalCode', 'ClosePrice']]\n",
        "    .groupby('PostalCode', as_index=False)['ClosePrice'].median()\n",
        "    .rename(columns={'ClosePrice':'PC_MedianPrice'})\n",
        ")\n",
        "\n",
        "# 2) Bin medians into k quantile clusters (no lat/lon)\n",
        "k = 10\n",
        "# if too few unique postals, reduce k\n",
        "k = min(k, pc_price['PC_MedianPrice'].nunique())\n",
        "labels = pd.qcut(pc_price['PC_MedianPrice'], q=k, labels=False, duplicates='drop')\n",
        "pc_price['GeoCluster'] = labels.astype(int)\n",
        "\n",
        "# 3) Map PostalCode → cluster\n",
        "PC2CLUSTER = dict(zip(pc_price['PostalCode'], pc_price['GeoCluster']))\n",
        "\n",
        "def assign_cluster_postal(df, mapping, unknown_label=-1):\n",
        "    lab = df['PostalCode'].map(mapping)\n",
        "    return lab.fillna(unknown_label).astype(int)\n",
        "\n",
        "train_df['GeoCluster'] = assign_cluster_postal(train_df, PC2CLUSTER)\n",
        "test_df['GeoCluster']  = assign_cluster_postal(test_df,  PC2CLUSTER)\n",
        "\n",
        "# 4) One-hot encode and align\n",
        "train_df = pd.get_dummies(train_df, columns=['GeoCluster'], prefix='GC', drop_first=True)\n",
        "test_df  = pd.get_dummies(test_df,  columns=['GeoCluster'], prefix='GC', drop_first=True)\n",
        "test_df  = test_df.reindex(columns=train_df.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "FEATURE_COLS = list(X_train.columns)\n",
        "\n",
        "with open(\"model_columns.json\", \"w\") as f:\n",
        "    json.dump({\"columns\": FEATURE_COLS}, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label: 6\n",
            "False\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ],
      "source": [
        "# check the label for 91344\n",
        "label = PC2CLUSTER[\"91344\"]\n",
        "print(\"label:\", label)\n",
        "\n",
        "# does its column exist?\n",
        "print(f'GC_{label}' in FEATURE_COLS)\n",
        "\n",
        "# which GC columns exist?\n",
        "print(sorted([int(c.split('_')[1]) for c in FEATURE_COLS if c.startswith('GC_')])[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.cluster import KMeans\n",
        "\n",
        "# # --- 0) Clean PostalCode and coords (coords only used to learn clusters) ---\n",
        "# for d in (train_df, test_df):\n",
        "#     d['PostalCode'] = d['PostalCode'].astype(str).str.strip().str.upper().replace({'': np.nan})\n",
        "#     d['Latitude']   = pd.to_numeric(d['Latitude'],  errors='coerce')\n",
        "#     d['Longitude']  = pd.to_numeric(d['Longitude'], errors='coerce')\n",
        "\n",
        "# # --- 1) Build postal-code centroids (TRAIN ONLY) ---\n",
        "# pc_xy = (\n",
        "#     train_df\n",
        "#     .loc[train_df['PostalCode'].notna() & train_df['Latitude'].notna() & train_df['Longitude'].notna(),\n",
        "#          ['PostalCode','Latitude','Longitude']]\n",
        "#     .groupby('PostalCode', as_index=False)\n",
        "#     .mean()\n",
        "# )\n",
        "\n",
        "# # Guard: need at least k distinct postal codes with coords\n",
        "# k = 10\n",
        "# if pc_xy.shape[0] < k:\n",
        "#     k = max(2, pc_xy.shape[0])\n",
        "\n",
        "# # Project lon to x using cosine(lat) scaling to reduce distortion\n",
        "# mean_lat = np.deg2rad(pc_xy['Latitude'].mean())\n",
        "# lon_scale = np.cos(mean_lat)\n",
        "# pc_xy['x'] = pc_xy['Longitude'] * lon_scale\n",
        "# pc_xy['y'] = pc_xy['Latitude']\n",
        "\n",
        "# # --- 2) Fit KMeans on postal-code centroids (TRAIN ONLY) ---\n",
        "# kmeans_pc = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "# kmeans_pc.fit(pc_xy[['x','y']])\n",
        "\n",
        "# # Each postal code's cluster label\n",
        "# pc_xy['GeoCluster'] = kmeans_pc.predict(pc_xy[['x','y']])\n",
        "\n",
        "# # Map PostalCode -> cluster\n",
        "# PC2CLUSTER = dict(zip(pc_xy['PostalCode'], pc_xy['GeoCluster']))\n",
        "\n",
        "# # --- 3) Assign clusters by PostalCode only (works at inference) ---\n",
        "# def assign_cluster_postal(df, mapping, unknown_label=-1):\n",
        "#     labels = df['PostalCode'].map(mapping)\n",
        "#     return labels.fillna(unknown_label).astype(int)\n",
        "\n",
        "# train_df['GeoCluster'] = assign_cluster_postal(train_df, PC2CLUSTER)\n",
        "# test_df['GeoCluster']  = assign_cluster_postal(test_df,  PC2CLUSTER)\n",
        "\n",
        "# # --- 4) One-hot encode cluster and align columns ---\n",
        "# train_df = pd.get_dummies(train_df, columns=['GeoCluster'], prefix='GC', drop_first=True)\n",
        "# test_df  = pd.get_dummies(test_df,  columns=['GeoCluster'], prefix='GC', drop_first=True)\n",
        "# test_df  = test_df.reindex(columns=train_df.columns, fill_value=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols = ['Flooring','LotSizeSquareFeet','LotSizeArea','StreetNumberNumeric','HighSchoolDistrict','Latitude','Longitude','PostalCode']\n",
        "train_df.drop(columns=cols, inplace=True, errors='ignore')\n",
        "test_df.drop(columns=cols, inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df['Age'] = 2025 - train_df['YearBuilt']\n",
        "test_df['Age']  = 2025 - test_df['YearBuilt']\n",
        "train_df.drop(columns=['YearBuilt'], inplace=True, errors='ignore')\n",
        "test_df.drop(columns=['YearBuilt'], inplace=True, errors='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['ViewYN', 'PoolPrivateYN', 'CloseDate', 'ClosePrice', 'UnparsedAddress',\n",
              "       'PropertyType', 'LivingArea', 'CountyOrParish', 'AttachedGarageYN',\n",
              "       'ParkingTotal', 'PropertySubType', 'LotSizeAcres',\n",
              "       'BathroomsTotalInteger', 'City', 'BedroomsTotal', 'StateOrProvince',\n",
              "       'FireplaceYN', 'Stories', 'Levels', 'MainLevelBedrooms',\n",
              "       'NewConstructionYN', 'GarageSpaces', 'AssociationFee', 'GC_1', 'GC_2',\n",
              "       'GC_3', 'GC_4', 'GC_5', 'GC_6', 'GC_7', 'GC_8', 'GC_9', 'Age'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 feature        VIF\n",
            "4          BedroomsTotal  23.163606\n",
            "3  BathroomsTotalInteger  22.061415\n",
            "0             LivingArea  16.137429\n",
            "5                Stories   9.596321\n",
            "9                    Age   3.433973\n",
            "6      MainLevelBedrooms   2.868417\n",
            "7           GarageSpaces   1.451653\n",
            "8         AssociationFee   1.257817\n",
            "1           ParkingTotal   1.004705\n",
            "2           LotSizeAcres   1.000341\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "target = 'ClosePrice'\n",
        "\n",
        "# 1) numeric-only, exclude target\n",
        "num_df = train_df.select_dtypes(include=['number']).drop(columns=[target], errors='ignore')\n",
        "\n",
        "# 2) force float dtype (handles Int64/booleans/nullable)\n",
        "num_df = num_df.apply(pd.to_numeric, errors='coerce').astype('float64')\n",
        "\n",
        "# 3) drop non-finite rows\n",
        "num_df = num_df.replace([np.inf, -np.inf], np.nan).dropna(axis=0, how='any')\n",
        "\n",
        "# 4) drop zero-variance columns (VIF undefined)\n",
        "vt = VarianceThreshold(threshold=0.0)\n",
        "_ = vt.fit(num_df)\n",
        "num_df = num_df.loc[:, vt.get_feature_names_out(num_df.columns)]\n",
        "\n",
        "# 5) compute VIF\n",
        "X = num_df.values  # pure float64\n",
        "vif = pd.DataFrame({\n",
        "    \"feature\": num_df.columns,\n",
        "    \"VIF\": [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
        "}).sort_values(\"VIF\", ascending=False)\n",
        "\n",
        "print(vif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_absolute_percentage_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "keep_cols = [\n",
        "    'LivingArea', 'ParkingTotal', 'LotSizeAcres', 'BathroomsTotalInteger', 'ClosePrice',\n",
        "    'City', 'BedroomsTotal','Stories',\n",
        "    'MainLevelBedrooms', 'GarageSpaces','Age'\n",
        "] + [c for c in train_df.columns if c.startswith('GC_')]\n",
        "\n",
        "train_df = train_df[keep_cols].copy()\n",
        "test_df  = test_df[keep_cols].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R²: 0.8643618908545193\n",
            "MAPE: 0.15719391857507187\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Trim outliers (0.5% from top and bottom) ---\n",
        "def trim_quantile(df, target='ClosePrice', q=0.005):\n",
        "    lo = df[target].quantile(q)\n",
        "    hi = df[target].quantile(1 - q)\n",
        "    return df[(df[target] >= lo) & (df[target] <= hi)].copy()\n",
        "\n",
        "train_t = trim_quantile(train_df, q=0.005)\n",
        "test_t  = trim_quantile(test_df,  q=0.005)\n",
        "\n",
        "# --- 2. Split features and target ---\n",
        "target = 'ClosePrice'\n",
        "X_train = train_t.drop(columns=[target, 'CloseDate', 'UnparsedAddress'], errors='ignore')\n",
        "y_train = train_t[target]\n",
        "X_test  = test_t.drop(columns=[target, 'CloseDate', 'UnparsedAddress'], errors='ignore')\n",
        "y_test  = test_t[target]\n",
        "\n",
        "# --- 3. One-hot encode categorical features ---\n",
        "cat_cols = X_train.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
        "X_train = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)\n",
        "X_test  = pd.get_dummies(X_test,  columns=cat_cols, drop_first=True)\n",
        "X_test  = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "\n",
        "# --- 4. Model training ---\n",
        "model = XGBRegressor(\n",
        "    n_estimators=600,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    tree_method='hist',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# y_train_log = np.log1p(y_train)\n",
        "# model.fit(X_train, y_train_log)\n",
        "# y_pred = np.expm1(model.predict(X_test))\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 5. Evaluation ---\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"R²:\", metrics.r2_score(y_test, y_pred))\n",
        "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Median Absolute Percentage Error (MdAPE): 11.00%\n"
          ]
        }
      ],
      "source": [
        "def calculate_mdape(y_true, y_pred):\n",
        "    \"\"\"Calculates the Median Absolute Percentage Error (MdAPE).\"\"\"\n",
        "    # Exclude cases where the actual value is zero to avoid division by zero\n",
        "    non_zero_indices = y_true != 0\n",
        "    y_true_filtered = y_true[non_zero_indices]\n",
        "    y_pred_filtered = y_pred[non_zero_indices]\n",
        "\n",
        "    # Calculate Absolute Percentage Error (APE)\n",
        "    ape = np.abs((y_true_filtered - y_pred_filtered) / y_true_filtered)\n",
        "\n",
        "    # Return the median of the APE\n",
        "    return np.median(ape) * 100 # Return as a percentage\n",
        "\n",
        "# Calculate and print the MdAPE\n",
        "mdape_score = calculate_mdape(y_test, y_pred)\n",
        "print(f\"Median Absolute Percentage Error (MdAPE): {mdape_score:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20a4ff2ec48d4a7f82bee329623be86e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(Dropdown(description='City:', options=('29 Palms', 'Acton', 'Adelanto', 'Agoura Hills', …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import ipywidgets as w\n",
        "\n",
        "# Build plotting frame (align with your test set)\n",
        "plot_df = test_t[['City']].copy()\n",
        "plot_df['Actual']    = y_test.values\n",
        "plot_df['Predicted'] = y_pred\n",
        "plot_df = plot_df.dropna(subset=['City'])\n",
        "\n",
        "def plot_city(city):\n",
        "    sub = plot_df.loc[plot_df['City'] == city].copy()\n",
        "    if sub.empty:\n",
        "        print(\"No rows for\", city); return\n",
        "    sub = sub.sort_values('Actual').reset_index(drop=True)\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(y=sub['Actual'], mode='lines', name='Actual ClosePrice'))\n",
        "    fig.add_trace(go.Scatter(y=sub['Predicted'], mode='lines', name='Predicted ClosePrice'))\n",
        "    fig.update_layout(\n",
        "        title=f\"Actual vs Predicted — {city}\",\n",
        "        xaxis_title=\"Samples (sorted by actual value)\",\n",
        "        yaxis_title=\"ClosePrice\",\n",
        "        hovermode=\"x unified\"\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "city_dropdown = w.Dropdown(options=sorted(plot_df['City'].unique()), description='City:')\n",
        "w.interact(plot_city, city=city_dropdown);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "import json\n",
        "\n",
        "# Save model\n",
        "with open(\"xgb_houseprice.pkl\", \"wb\") as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Save feature columns\n",
        "with open(\"model_columns.json\", \"w\") as f:\n",
        "    json.dump({\"columns\": list(X_train.columns)}, f)\n",
        "\n",
        "# Save postal code → cluster map\n",
        "with open(\"pc2cluster.pkl\", \"wb\") as f:\n",
        "    pickle.dump(PC2CLUSTER, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "# after you compute y_pred on test_t in training\n",
        "eval_df = test_t[['City']].copy()\n",
        "eval_df['Actual'] = y_test.values\n",
        "eval_df['Predicted'] = y_pred\n",
        "pickle.dump(eval_df, open(\"eval_df.pkl\",\"wb\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from xgboost import XGBRegressor\n",
        "# from sklearn import metrics\n",
        "# from joblib import dump, load\n",
        "# import os\n",
        "\n",
        "# # 4) Model with tuned hyperparams (no CV each run)\n",
        "# best_params = dict(\n",
        "#     objective='reg:squarederror',\n",
        "#     tree_method='hist',\n",
        "#     learning_rate=0.05,   # eta\n",
        "#     max_depth=6,\n",
        "#     min_child_weight=3,\n",
        "#     subsample=0.8,\n",
        "#     colsample_bytree=0.8,\n",
        "#     reg_lambda=1.5,       # lambda\n",
        "#     reg_alpha=0.1,        # alpha\n",
        "#     n_estimators=4221,\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# model_path = \"xgb_houseprice.joblib\"\n",
        "\n",
        "# # Optional: cache trained model to skip retraining\n",
        "# if os.path.exists(model_path):\n",
        "#     model = load(model_path)\n",
        "# else:\n",
        "#     model = XGBRegressor(**best_params)\n",
        "#     model.fit(X_train, y_train)\n",
        "#     dump(model, model_path)\n",
        "\n",
        "# # 5) Evaluation\n",
        "# y_pred = model.predict(X_test)\n",
        "# print(\"R²:\", metrics.r2_score(y_test, y_pred))\n",
        "# print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import xgboost as xgb\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
        "\n",
        "# # 1) Train/valid split once (to avoid expensive K-fold on huge data)\n",
        "# X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# dtr  = xgb.DMatrix(X_tr,  label=y_tr)\n",
        "# dval = xgb.DMatrix(X_val, label=y_val)\n",
        "\n",
        "# # 2) Base params\n",
        "# base = dict(\n",
        "#     objective='reg:squarederror',\n",
        "#     eval_metric='rmse',\n",
        "#     tree_method='hist',     # set to 'hist'; if GPU: tree_method='hist', device='cuda'\n",
        "#     # device='cuda',        # uncomment if GPU is available and XGBoost>=2.0\n",
        "#     nthread=-1\n",
        "# )\n",
        "\n",
        "# # 3) Small, sane search space\n",
        "# search_space = [\n",
        "#     {'max_depth':6, 'min_child_weight':3, 'subsample':0.8, 'colsample_bytree':0.8, 'lambda':1.5, 'alpha':0.1, 'eta':0.05},\n",
        "#     {'max_depth':8, 'min_child_weight':5, 'subsample':0.7, 'colsample_bytree':0.9, 'lambda':2.0, 'alpha':0.0, 'eta':0.05},\n",
        "#     {'max_depth':10,'min_child_weight':7, 'subsample':0.8, 'colsample_bytree':0.8, 'lambda':2.0, 'alpha':0.3, 'eta':0.03},\n",
        "#     {'max_depth':8, 'min_child_weight':3, 'subsample':0.9, 'colsample_bytree':0.7, 'lambda':1.0, 'alpha':0.0, 'eta':0.1},\n",
        "#     {'max_depth':6, 'min_child_weight':1, 'subsample':0.6, 'colsample_bytree':1.0, 'lambda':3.0, 'alpha':0.5, 'eta':0.03},\n",
        "# ]\n",
        "\n",
        "# best = None\n",
        "# best_score = np.inf\n",
        "# best_ntrees = None\n",
        "\n",
        "# # 4) Loop with early stopping\n",
        "# for i, hp in enumerate(search_space, 1):\n",
        "#     params = base.copy()\n",
        "#     params.update(hp)\n",
        "\n",
        "#     evals_result = {}\n",
        "#     booster = xgb.train(\n",
        "#         params,\n",
        "#         dtr,\n",
        "#         num_boost_round=5000,                # large ceiling\n",
        "#         evals=[(dtr,'train'), (dval,'valid')],\n",
        "#         early_stopping_rounds=100,\n",
        "#         evals_result=evals_result,\n",
        "#         verbose_eval=False\n",
        "#     )\n",
        "\n",
        "#     rmse = booster.best_score\n",
        "#     if rmse < best_score:\n",
        "#         best_score = rmse\n",
        "#         best = params\n",
        "#         best_ntrees = booster.best_iteration + 1\n",
        "\n",
        "# print(\"Best params:\", best)\n",
        "# print(\"Valid RMSE:\", best_score, \"Best trees:\", best_ntrees)\n",
        "\n",
        "# # 5) Retrain on ALL training data with best params and best_ntrees\n",
        "# dall = xgb.DMatrix(X_train, label=y_train)\n",
        "# final_model = xgb.train(best, dall, num_boost_round=best_ntrees)\n",
        "\n",
        "# # 6) Evaluate on test\n",
        "# dtest = xgb.DMatrix(X_test)\n",
        "# y_pred = final_model.predict(dtest)\n",
        "# print(\"Test R²:\", r2_score(y_test, y_pred))\n",
        "# print(\"Test MAPE:\", mean_absolute_percentage_error(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.metrics import make_scorer, r2_score\n",
        "\n",
        "# # Base model\n",
        "# xgb_base = XGBRegressor(\n",
        "#     tree_method='hist',\n",
        "#     random_state=42,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# # Parameter grid (balanced for runtime and performance)\n",
        "# param_grid = {\n",
        "#     'n_estimators': [300, 600, 900],\n",
        "#     'learning_rate': [0.01, 0.05, 0.1],\n",
        "#     'max_depth': [6, 8, 10],\n",
        "#     'min_child_weight': [1, 3, 5],\n",
        "#     'subsample': [0.7, 0.8, 1.0],\n",
        "#     'colsample_bytree': [0.7, 0.8, 1.0],\n",
        "#     'reg_lambda': [1, 1.5, 2],\n",
        "#     'reg_alpha': [0, 0.1, 0.3]\n",
        "# }\n",
        "\n",
        "# # Define scorer (R²)\n",
        "# scorer = make_scorer(r2_score, greater_is_better=True)\n",
        "\n",
        "# # GridSearchCV setup\n",
        "# grid = GridSearchCV(\n",
        "#     estimator=xgb_base,\n",
        "#     param_grid=param_grid,\n",
        "#     scoring=scorer,\n",
        "#     cv=3,\n",
        "#     verbose=2,\n",
        "#     n_jobs=-1\n",
        "# )\n",
        "\n",
        "# # Fit grid search\n",
        "# grid.fit(X_train, y_train)\n",
        "\n",
        "# print(\"Best Parameters:\", grid.best_params_)\n",
        "# print(\"Best CV R²:\", grid.best_score_)\n",
        "\n",
        "# # Evaluate best model on test data\n",
        "# best_xgb = grid.best_estimator_\n",
        "# y_pred = best_xgb.predict(X_test)\n",
        "\n",
        "# print(\"Test R²:\", r2_score(y_test, y_pred))\n",
        "# print(\"Test MAPE:\", mean_absolute_percentage_error(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbmi5aTZLSQ7"
      },
      "source": [
        "----------train and testing-------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "k2493fCiLYKA"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# target = \"ClosePrice\"\n",
        "\n",
        "# # features from train\n",
        "# num_cols = train_df.select_dtypes(include=[np.number]).columns.drop(target, errors=\"ignore\").tolist()\n",
        "\n",
        "# # cutoffs from train (0.5% tails)\n",
        "# y_tr_all = pd.to_numeric(train_df[target], errors=\"coerce\")\n",
        "# lo, hi = y_tr_all.quantile([0.005, 0.995])\n",
        "\n",
        "# # trim train\n",
        "# train_trim = train_df[(y_tr_all >= lo) & (y_tr_all <= hi)].copy()\n",
        "\n",
        "# # trim test using same cutoffs\n",
        "# y_te_all = pd.to_numeric(test_df[target], errors=\"coerce\")\n",
        "# test_trim = test_df[(y_te_all >= lo) & (y_te_all <= hi)].copy()\n",
        "\n",
        "# # train/valid split\n",
        "# X = train_trim[num_cols].astype(\"float32\")\n",
        "# y = pd.to_numeric(train_trim[target], errors=\"coerce\").astype(\"float32\")\n",
        "# X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # test matrices\n",
        "# X_test = test_trim.reindex(columns=num_cols, fill_value=0).astype(\"float32\")\n",
        "# y_test = pd.to_numeric(test_trim[target], errors=\"coerce\").astype(\"float32\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "XrXAmAiBLYKC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Generalist\n",
        "# xgbr = XGBRegressor(\n",
        "#     n_estimators=1200, learning_rate=0.03,\n",
        "#     max_depth=6, min_child_weight=5,\n",
        "#     subsample=0.8, colsample_bytree=0.8,\n",
        "#     reg_lambda=2.0, reg_alpha=0.0, gamma=0.0,\n",
        "#     objective=\"reg:squarederror\", tree_method=\"hist\",\n",
        "#     n_jobs=-1, random_state=42\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "KtiFhZF8LYKC"
      },
      "outputs": [],
      "source": [
        "# xgbr.fit(X_tr, y_tr)\n",
        "# pred_va = xgbr.predict(X_va)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open(\"xgb_model.pkl\", \"wb\") as f:\n",
        "#     pickle.dump(xgbr, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4plw_NCLYKC",
        "outputId": "1f08615d-a205-4fde-c84b-ceb5e2d92b9b"
      },
      "outputs": [],
      "source": [
        "# from sklearn import metrics\n",
        "# print('R2 Score',metrics.r2_score(y_va,pred_va))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMSk4d1SLYKD",
        "outputId": "02bef4ee-e9fe-474b-87fa-fdecfb29d9cf"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import mean_absolute_percentage_error\n",
        "# mape = mean_absolute_percentage_error(y_va, pred_va)\n",
        "\n",
        "# print(f'MAPE: {mape:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQKc5JnJLYKD",
        "outputId": "7121d760-1f08-4cb5-fae2-61ca7566fee4"
      },
      "outputs": [],
      "source": [
        "# def calculate_mdape(y_true, y_pred):\n",
        "#     \"\"\"Calculates the Median Absolute Percentage Error (MdAPE).\"\"\"\n",
        "#     # Exclude cases where the actual value is zero to avoid division by zero\n",
        "#     non_zero_indices = y_true != 0\n",
        "#     y_true_filtered = y_true[non_zero_indices]\n",
        "#     y_pred_filtered = y_pred[non_zero_indices]\n",
        "\n",
        "#     # Calculate Absolute Percentage Error (APE)\n",
        "#     ape = np.abs((y_true_filtered - y_pred_filtered) / y_true_filtered)\n",
        "\n",
        "#     # Return the median of the APE\n",
        "#     return np.median(ape) * 100 # Return as a percentage\n",
        "\n",
        "# # Calculate and print the MdAPE\n",
        "# mdape_score = calculate_mdape(y_va, pred_va)\n",
        "# print(f\"Median Absolute Percentage Error (MdAPE): {mdape_score:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO6Nfk_n2on6",
        "outputId": "dd2d8426-3bde-4c1a-d81f-c9a9ee87eb7a"
      },
      "outputs": [],
      "source": [
        "# importances = xgbr.feature_importances_\n",
        "# sorted(zip(importances, num_cols), reverse=True)[:20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "# artifacts = {\n",
        "#     \"NUM_COLS\": num_cols,          # final numeric feature list used to train XGB\n",
        "#     \"MODE_LEVELS\": mode_val,       # if you still use it anywhere\n",
        "#     \"KNN_COLS\": [\"LivingArea\",\"LotSizeAcres\",\"LotSizeArea\",\"AssociationFee\",\"LotSizeSquareFeet\"],\n",
        "#     \"NUM_BLOCK\": [\"YearBuilt\",\"BathroomsTotalInteger\",\"BedroomsTotal\",\"ParkingTotal\",\"MainLevelBedrooms\",\"GarageSpaces\"]\n",
        "# }\n",
        "# with open(\"xgb_model.pkl\",\"wb\") as f: pickle.dump(xgbr,f)\n",
        "# with open(\"num_imputer.pkl\",\"wb\") as f: pickle.dump(num_imputer,f)\n",
        "# with open(\"scaler.pkl\",\"wb\") as f: pickle.dump(scaler,f)\n",
        "# with open(\"knn_imp.pkl\",\"wb\") as f: pickle.dump(imp,f)\n",
        "# with open(\"artifacts.pkl\",\"wb\") as f: pickle.dump(artifacts,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
